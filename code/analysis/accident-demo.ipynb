{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters to run demo, change these as needed\n",
    "\n",
    "frate = 2.0 # choose either 2.0 or 5.0\n",
    "fname_in = 'test_video.avi'\n",
    "\n",
    "import sys\n",
    "if ((frate != 2.0) and (frate != 5.0)):\n",
    "    print 'Current analysis must be done with 2.0 or 5.0 fps'\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this reads in the avi\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import sys\n",
    "# you might to run this if you don't have ffmpeg.exe\n",
    "#imageio.plugins.ffmpeg.download()\n",
    "\n",
    "home = os.getcwd()\n",
    "fname_full = home + '/' + fname_in\n",
    "vid = imageio.get_reader(fname_full, 'ffmpeg')\n",
    "vid_info = vid.get_meta_data()\n",
    "s_hz = vid_info['fps']\n",
    "s_length = vid_info['duration']\n",
    "\n",
    "min_duration = 1.0/frate\n",
    "if (s_length < min_duration):\n",
    "    print 'the video is too short'\n",
    "    print 'the minimum duration is ',min_duration\n",
    "    print 'and the length of this video is ',duration\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 40\n"
     ]
    }
   ],
   "source": [
    "# This takes the frames of the avi and converts them into the .hkl files\n",
    "# The script is based on process_kitti\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "import hickle as hkl\n",
    "import pandas as pd\n",
    "\n",
    "desired_im_sz = (128, 160)\n",
    "\n",
    "# resize and crop image\n",
    "def process_im(im, desired_sz):\n",
    "    target_ds = float(desired_sz[0])/im.shape[0]\n",
    "    im = imresize(im, (desired_sz[0], int(np.round(target_ds * im.shape[1]))))\n",
    "    d = (im.shape[1] - desired_sz[1]) / 2\n",
    "    im = im[:, d:d+desired_sz[1]]\n",
    "    return im\n",
    "\n",
    "split = 'test'\n",
    "frames_per_ex = 10\n",
    "max_clip = 100 # a limit to the batch file size to run on my container\n",
    "\n",
    "# calculate num_im,skip, parts, step_im\n",
    "s_num_frames = int(round(s_hz * s_length))\n",
    "s_orig_ms_per_frame = 1000/s_hz\n",
    "s_targ_ms_per_frame = 1000.0/frate\n",
    "skip = int(s_targ_ms_per_frame/s_orig_ms_per_frame)\n",
    "s_actual_hz = s_hz/skip\n",
    "s_actual_ms_per_frame = 1000/s_actual_hz\n",
    "s_total_clips = int(s_num_frames/(frames_per_ex*skip))\n",
    "parts = int(s_total_clips/max_clip) + 1\n",
    "step_im = max_clip * frames_per_ex\n",
    "suffix = '_P'+str(parts)+'_'\n",
    "num_im = s_total_clips*frames_per_ex \n",
    "\n",
    "X = np.zeros((num_im,) + desired_im_sz + (3,), np.uint8)\n",
    "source_list = [fname_in]*num_im\n",
    "ct = 0\n",
    "for i,im_out in enumerate(vid):\n",
    "    if (i % skip == 0):\n",
    "        im = vid.get_data(i)\n",
    "        #print i\n",
    "        X[ct] = process_im(im, desired_im_sz)\n",
    "        ct = ct + 1\n",
    "        if (ct == num_im):\n",
    "            break\n",
    "\n",
    "if not (os.path.isdir(home+\"/test_avi\")):\n",
    "    os.mkdir(home+'/test_avi')\n",
    "else:\n",
    "    allfiles =os.listdir(home+'/test_avi')\n",
    "    for temp in allfiles:\n",
    "        os.remove(home+'/test_avi'+'/'+temp)\n",
    "    os.rmdir(home+'/test_avi')\n",
    "    os.mkdir(home+'/test_avi')\n",
    "    \n",
    "for part in range(1,parts+1):\n",
    "    xbeg = int((part-1)*step_im)\n",
    "    if (part == parts):\n",
    "        xend = num_im    \n",
    "    else:\n",
    "        xend = int(part*step_im) \n",
    "    print part,xbeg,xend\n",
    "    hkl.dump(X[xbeg:xend], os.path.join(home,'test_avi', 'X_' + split + suffix+ str(part)+'.hkl'))\n",
    "    hkl.dump(source_list[xbeg:xend], os.path.join(home,'test_avi', 'sources_' + split + suffix + str(part)+ '.hkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_frame2\n",
      "<type 'numpy.ndarray'>\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluate trained PredNet on test video.\n",
    "Calculates mean-squared error\n",
    "\n",
    "SS 12/5/2016:\n",
    "modified to read in batched .hkl files\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from six.moves import cPickle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "\n",
    "from prednet import PredNet\n",
    "from data_utils import SequenceGenerator\n",
    "#from kitti_settings_ss import *\n",
    "DATA_DIR = './test_avi/'\n",
    "WEIGHTS_DIR = './model_data/'\n",
    "RESULTS_SAVE_DIR = './test_results_avi/'\n",
    "\n",
    "# these are set in the cell above, uncomment this and this cell should\n",
    "# work for avi's less than 1000 frames (1000 clips at 10 frames/clip)\n",
    "# This would be 500 sec at 2Hz or 200 sec at 5Hz \n",
    "#parts = 1\n",
    "#suffix = '_P1_'\n",
    "\n",
    "n_plot = 40\n",
    "batch_size = 10\n",
    "nt = 10\n",
    "\n",
    "weights_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_weights.hdf5')\n",
    "json_file = os.path.join(WEIGHTS_DIR, 'prednet_kitti_model.json')\n",
    "\n",
    "# Load trained model\n",
    "f = open(json_file, 'r')\n",
    "json_string = f.read()\n",
    "f.close()\n",
    "train_model = model_from_json(json_string, custom_objects = {'PredNet': PredNet})\n",
    "train_model.load_weights(weights_file)\n",
    "\n",
    "# Create testing model (to output predictions)\n",
    "layer_config = train_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "dim_ordering = layer_config['dim_ordering']\n",
    "test_prednet = PredNet(weights=train_model.layers[1].get_weights(), **layer_config)\n",
    "input_shape = list(train_model.layers[0].batch_input_shape[1:])\n",
    "input_shape[0] = nt\n",
    "inputs = Input(shape=tuple(input_shape))\n",
    "predictions = test_prednet(inputs)\n",
    "test_model = Model(input=inputs, output=predictions)\n",
    "if not os.path.exists(RESULTS_SAVE_DIR): os.mkdir(RESULTS_SAVE_DIR)\n",
    "f = open(RESULTS_SAVE_DIR + 'prediction_scores.txt', 'w')\n",
    "\n",
    "for part in range(1,parts+1):\n",
    "\n",
    "    curr_test = 'X_test'+suffix+str(part)+'.hkl'\n",
    "    curr_sources = 'sources_test'+suffix+str(part)+'.hkl'\n",
    "    test_file = os.path.join(DATA_DIR, curr_test)\n",
    "    test_sources = os.path.join(DATA_DIR, curr_sources)\n",
    "\n",
    "    test_generator = SequenceGenerator(test_file, test_sources, nt, sequence_start_mode='unique', dim_ordering=dim_ordering)\n",
    "    X_test = test_generator.create_all()\n",
    "    X_hat = test_model.predict(X_test, batch_size)\n",
    "    if dim_ordering == 'th':\n",
    "        X_test = np.transpose(X_test, (0, 1, 3, 4, 2))\n",
    "        X_hat = np.transpose(X_hat, (0, 1, 3, 4, 2))\n",
    "\n",
    "    curr_mse_frame2 = 'mse_frame2'+suffix+str(part)+'.csv'\n",
    "    mse_frame2_out = os.path.join(RESULTS_SAVE_DIR, curr_mse_frame2)\n",
    "    \n",
    "    mse_point = (X_test[:, 1:] - X_hat[:, 1:])**2\n",
    "    mse_frame2 = np.squeeze(np.apply_over_axes(np.mean, mse_point, [2,3,4]))\n",
    "    np.savetxt(mse_frame2_out, mse_frame2, delimiter=\",\")\n",
    "\n",
    "    print 'mse_frame2'\n",
    "    print type(mse_frame2)\n",
    "    print mse_frame2.shape\n",
    "    \n",
    "    # Compare overall MSE's write results to prediction_scores.txt\n",
    "    mse_model = np.mean( (X_test[:, 1:] - X_hat[:, 1:])**2 )  # look at all timesteps except the first\n",
    "    mse_prev = np.mean( (X_test[:, :-1] - X_test[:, 1:])**2 )\n",
    "    mse_last = np.mean( (X_test[:, -1] - X_hat[:, -1])**2 )  # look only the last frame\n",
    "    f.write(\"part number: %d\\n\" % part)\n",
    "    f.write(\"Model MSE: %f\\n\" % mse_model)\n",
    "    f.write(\"Last Frame MSE: %f\" % mse_last)\n",
    "    f.write(\"Previous Frame MSE: %f\" % mse_prev)\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing possible accidents\n",
      "No possible accidents found\n"
     ]
    }
   ],
   "source": [
    "# see https://github.com/startupml/video/blob/master/results/README.md for rationale for these thresholds\n",
    "if (frate == 2.0):\n",
    "    threshold = 0.00777565 \n",
    "else: #frate == 5.0, as determined by first cell\n",
    "    threshold = 0.004869763 \n",
    "    \n",
    "mse_clip = pd.Series(np.squeeze(np.apply_over_axes(np.mean, mse_frame2, [1])))\n",
    "\n",
    "clip_size = mse_clip.shape[0]\n",
    "t_start = pd.Series(np.zeros(clip_size))\n",
    "t_end = pd.Series(np.zeros(clip_size))\n",
    "above_threshold = pd.Series(np.zeros(clip_size))\n",
    "\n",
    "for i in range(0,clip_size):\n",
    "    if (mse_clip[i] >= threshold):\n",
    "        above_threshold[i] = 1\n",
    "    t_start[i] = i*(frames_per_ex*skip*s_orig_ms_per_frame)\n",
    "    t_end[i] = (i+1)*(frames_per_ex*skip*s_orig_ms_per_frame) - (skip*s_orig_ms_per_frame)\n",
    "\n",
    "mse_clip_out = pd.concat([mse_clip,t_start,t_end,above_threshold],axis=1)\n",
    "mse_clip_out.columns = ['MSE_ave_over_clip','time_start','time_end','above_threshold']    \n",
    "mse_clip_out.to_csv(os.path.join(RESULTS_SAVE_DIR, 'test_avi_mse_clip.csv'))\n",
    "\n",
    "print 'Assessing possible accidents'\n",
    "if (np.sum(mse_clip_out.above_threshold) == 0):\n",
    "    print \"No possible accidents found\"\n",
    "else:\n",
    "    for i in range(0,clip_size):\n",
    "        if (mse_clip_out.above_threshold.iloc[i] == 1):\n",
    "            print 'Possible accident in clip ',i, 'starting time = ', mse_clip_out.time_start.iloc[i], \\\n",
    "            'ending time = ',mse_clip_out.time_end.iloc[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
